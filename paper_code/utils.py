# Copyright (c) 2022 Battelle Energy Alliance, LLC
# Licensed under MIT License, please see LICENSE for details
# https://github.com/IdahoLabResearch/BIhNNs/blob/main/LICENSE

# Training Hamiltonian Neural Networks (HNNs) for Bayesian inference problems
# Original authors of HNNs code: Sam Greydanus, Misko Dzamba, Jason Yosinski (2019)
# Available at https://github.com/greydanus/hamiltonian-nn under the Apache License 2.0
# Modified by Som Dhulipala at Idaho National Laboratory for Bayesian inference problems
# Modifications include:
# - Generalizing the code to any number of dimensions
# - Introduce latent parameters to HNNs to improve expressivity
# - Reliance on the leap frog integrator for improved dynamics stability
# - Obtain the training from probability distribution space
# - Use a deep HNN arichtecture to improve predictive performance

import sys
import autograd.numpy as np
import os, torch, pickle, zipfile
import imageio, shutil

def leapfrog ( dydt, tspan, y0, n, dim ):
  # dt: step size
  # n: number of steps
  # should be leapfrog integration scheme mentioned in Eq.7/8 of the paper
  t0 = tspan[0]
  tstop = tspan[1]
  dt = ( tstop - t0 ) / n

  t = np.zeros ( n + 1 )
  y = np.zeros ( [dim, n + 1] )

  for i in range ( 0, n + 1 ):

    if ( i == 0 ):
      t[0]   = t0
      for j in range ( 0, dim ):
          y[j,0] = y0[j]
      anew   = dydt ( t, y[:,i] )
    else:
      t[i]   = t[i-1] + dt
      aold   = anew
      for j in range ( 0, int(dim/2) ):
          # why there is no m_i?
          # and in the second term: why it is dt instead of dt**2
          y[j,i] = y[j,i-1] + dt * ( y[(j+int(dim/2)),i-1] + 0.5 * dt * aold[(j+int(dim/2))] )
      anew   = dydt ( t, y[:,i] )
      for j in range ( 0, int(dim/2) ):
          y[(j+int(dim/2)),i] = y[(j+int(dim/2)),i-1] + 0.5 * dt * ( aold[(j+int(dim/2))] + anew[(j+int(dim/2))] )
  return y #t,

# def integrate_model(model, t_span, y0, n, **kwargs):
#   def default_fun(t, np_x):
#       x = torch.tensor( np_x, requires_grad=True, dtype=torch.float32)
#       x = x.view(1, np.size(np_x)) # batch size of 1
#       dx = model.time_derivative(x).data.numpy().reshape(-1)
#       return dx
#   fun = default_fun # if fun is None else fun
#   return leapfrog(fun, t_span, y0, n, 10)

def lfrog(fun, y0, t, dt, *args, **kwargs):
  k1 = fun(y0, t-dt, *args, **kwargs)
  k2 = fun(y0, t+dt, *args, **kwargs)
  dy = (k2-k1) / (2*dt)
  return dy

def L2_loss(u, v):
  return (u-v).pow(2).mean()

def to_pickle(thing, path): # save something
    with open(path, 'wb') as handle:
        pickle.dump(thing, handle, protocol=pickle.HIGHEST_PROTOCOL)

def from_pickle(path): # load something
    thing = None
    with open(path, 'rb') as handle:
        thing = pickle.load(handle)
    return thing

def choose_nonlinearity(name):
  nl = None
  if name == 'tanh':
    nl = torch.tanh
  elif name == 'relu':
    nl = torch.relu
  elif name == 'sigmoid':
    nl = torch.sigmoid
  elif name == 'softplus':
    nl = torch.nn.functional.softplus
  elif name == 'selu':
    nl = torch.nn.functional.selu
  elif name == 'elu':
    nl = torch.nn.functional.elu
  elif name == 'swish':
    nl = lambda x: x * torch.sigmoid(x)
  elif name == 'sine':
    nl = lambda x: torch.sin(x)
  else:
    raise ValueError("nonlinearity not recognized")
  return nl

class Transcript(object):
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.logfile = open(filename, "a+")

    def write(self, message):
        self.terminal.write(message)
        self.logfile.write(message)

    def flush(self):
        # this flush method is needed for python 3 compatibility.
        # this handles the flush command by doing nothing.
        # you might want to specify some extra behavior here.
        pass

def log_start(filename):
    """Start transcript, appending print output to given filename"""
    sys.stdout = Transcript(filename)

def log_stop():
    """Stop transcript and return print functionality to normal"""
    sys.stdout.logfile.close()
    sys.stdout = sys.stdout.terminal